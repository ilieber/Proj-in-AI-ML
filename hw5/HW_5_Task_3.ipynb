{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Projects in AI and ML HW5 Task 3"
      ],
      "metadata": {
        "id": "BMM3Q4biaWce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q: Query matrix\n",
        "    K: Key matrix\n",
        "    V: Value matrix\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
        "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Example input\n",
        "Q = np.array([[1, 0, 1]])\n",
        "K = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\n",
        "V = np.array([[1, 2], [0, 3], [4, 5]])\n",
        "\n",
        "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention Output:\\n\", output)\n",
        "print(\"Attention Weights:\\n\", attn_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT9saQhqdlPb",
        "outputId": "4b2b20cb-e3ff-4ba5-e068-18a03022eada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Output:\n",
            " [[2.15968551 3.4319371 ]]\n",
            "Attention Weights:\n",
            " [[0.4319371 0.1361258 0.4319371]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Seq2Seq with Attention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden[-1].repeat(src_len, 1, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=0)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(hidden_dim * 2 + emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2 + emb_dim + hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        attn_weights = self.attention(hidden, encoder_outputs)\n",
        "        attn_weights = attn_weights.unsqueeze(1)\n",
        "        weighted = torch.bmm(attn_weights, encoder_outputs.permute(1, 0, 2))\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0), embedded.squeeze(0)), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "# Define model parameters\n",
        "INPUT_DIM, OUTPUT_DIM = 3000, 3000\n",
        "ENC_EMB_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT = 256, 256, 512, 2, 0.5\n",
        "\n",
        "attn = Attention(HID_DIM)\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, attn)\n"
      ],
      "metadata": {
        "id": "mAqiZYkndo_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Machine Translation Dataset and Training\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Prepare data\n",
        "src_texts = [example[\"translation\"][\"en\"] for example in train_data]\n",
        "trg_texts = [example[\"translation\"][\"fr\"] for example in train_data]\n",
        "\n",
        "print(f\"Loaded {len(src_texts)} sentence pairs for training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoKt9AbqdtsF",
        "outputId": "79fbb518-a086-49b2-8b01-74dfb28e793d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 127085 sentence pairs for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Simplified Transformer Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, trg_texts, tokenizer, max_len=50):\n",
        "        self.src_texts = src_texts\n",
        "        self.trg_texts = trg_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use the Hugging Face tokenizer directly\n",
        "        src = self.tokenizer(self.src_texts[idx], padding='max_length', max_length=self.max_len, truncation=True, return_tensors='pt').input_ids.squeeze()\n",
        "        trg = self.tokenizer(self.trg_texts[idx], padding='max_length', max_length=self.max_len, truncation=True, return_tensors='pt').input_ids.squeeze()\n",
        "        return src, trg\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, emb_dim=64, n_heads=2, n_layers=2, ff_dim=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.positional_encoding = self.get_positional_encoding(100, emb_dim)\n",
        "        self.encoder_layers = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layers, num_layers=n_layers)\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "\n",
        "    def get_positional_encoding(self, max_len, emb_dim):\n",
        "        position = np.arange(max_len)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, emb_dim, 2) * -(np.log(10000.0) / emb_dim))\n",
        "        pe = np.zeros((max_len, emb_dim))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        return torch.tensor(pe, dtype=torch.float)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src) + self.positional_encoding[:src.size(0), :]\n",
        "        encoder_output = self.encoder(embedded)\n",
        "        output = self.fc_out(encoder_output)\n",
        "        return output\n",
        "\n",
        "transformer = Transformer(INPUT_DIM, OUTPUT_DIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KJiNKW3dwmo",
        "outputId": "93a8e6d2-cc9e-4beb-a5f4-dfa7fb3ff0fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Transformer to Dataset and BLEU Score Evaluation\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    return sentence_bleu(reference, candidate)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\") # or any other suitable tokenizer\n",
        "\n",
        "dataset = TranslationDataset(src_texts[:1000], trg_texts[:1000], tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    src_batch, trg_batch = batch\n",
        "    output = transformer(src_batch)\n",
        "    pred_sentences = [\"\".join(map(str, sent)) for sent in output.argmax(-1).tolist()]\n",
        "    ref_sentences = [\"\".join(map(str, sent)) for sent in trg_batch.tolist()]\n",
        "    bleu_scores = [calculate_bleu(ref, pred) for ref, pred in zip(ref_sentences, pred_sentences)]\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qi-qetIegp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, Transformers might perform better due to its ability to capture long-range dependencies, but it could also be worse if not trained sufficiently. Additionally, the Transformer could have longer runtime but faster inference time.\n"
      ],
      "metadata": {
        "id": "WophGPsPfVTz"
      }
    }
  ]
}